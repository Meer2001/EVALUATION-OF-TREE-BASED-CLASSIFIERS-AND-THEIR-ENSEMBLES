{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1270936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Decision Tree Experiments on CNF Datasets\n",
      "============================================================\n",
      "Data directory: c:\\utd\\sem1\\machinelearning\\project2\\project2_data\\all_data\n",
      "\n",
      "Scanning for available datasets...\n",
      "------------------------------------------------------------\n",
      "✓ Found: 300 clauses, 100 examples\n",
      "✓ Found: 300 clauses, 1000 examples\n",
      "✓ Found: 300 clauses, 5000 examples\n",
      "✓ Found: 500 clauses, 100 examples\n",
      "✓ Found: 500 clauses, 1000 examples\n",
      "✓ Found: 500 clauses, 5000 examples\n",
      "✓ Found: 1000 clauses, 100 examples\n",
      "✓ Found: 1000 clauses, 1000 examples\n",
      "✓ Found: 1000 clauses, 5000 examples\n",
      "✓ Found: 1500 clauses, 100 examples\n",
      "✓ Found: 1500 clauses, 1000 examples\n",
      "✓ Found: 1500 clauses, 5000 examples\n",
      "✓ Found: 1800 clauses, 100 examples\n",
      "✓ Found: 1800 clauses, 1000 examples\n",
      "✓ Found: 1800 clauses, 5000 examples\n",
      "------------------------------------------------------------\n",
      "Total available datasets: 15/15\n",
      "\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 100 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (200, 500)\n",
      "  Valid size: (200, 500)\n",
      "  Test size: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.5941\n",
      "  Validation F1: 0.5300\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.5350\n",
      "    Test F1 Score: 0.5507\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 1000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (2000, 500)\n",
      "  Valid size: (2000, 500)\n",
      "  Test size: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.6430\n",
      "  Validation F1: 0.6340\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.6610\n",
      "    Test F1 Score: 0.6558\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 5000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (10000, 500)\n",
      "  Valid size: (10000, 500)\n",
      "  Test size: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.7353\n",
      "  Validation F1: 0.7449\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.7793\n",
      "    Test F1 Score: 0.7874\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 100 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (200, 500)\n",
      "  Valid size: (200, 500)\n",
      "  Test size: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.6485\n",
      "  Validation F1: 0.5938\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.5200\n",
      "    Test F1 Score: 0.5102\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 1000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (2000, 500)\n",
      "  Valid size: (2000, 500)\n",
      "  Test size: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.6831\n",
      "  Validation F1: 0.6747\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.6760\n",
      "    Test F1 Score: 0.6667\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 5000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (10000, 500)\n",
      "  Valid size: (10000, 500)\n",
      "  Test size: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.7423\n",
      "  Validation F1: 0.7625\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.7883\n",
      "    Test F1 Score: 0.7981\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 100 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (200, 500)\n",
      "  Valid size: (200, 500)\n",
      "  Test size: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.7800\n",
      "  Validation F1: 0.7330\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.7450\n",
      "    Test F1 Score: 0.7437\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 1000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (2000, 500)\n",
      "  Valid size: (2000, 500)\n",
      "  Test size: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.7900\n",
      "  Validation F1: 0.7839\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.7945\n",
      "    Test F1 Score: 0.7966\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 5000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (10000, 500)\n",
      "  Valid size: (10000, 500)\n",
      "  Test size: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.8409\n",
      "  Validation F1: 0.8471\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.8597\n",
      "    Test F1 Score: 0.8646\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 100 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (200, 500)\n",
      "  Valid size: (200, 500)\n",
      "  Test size: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.8318\n",
      "  Validation F1: 0.8111\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.8450\n",
      "    Test F1 Score: 0.8442\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 1000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (2000, 500)\n",
      "  Valid size: (2000, 500)\n",
      "  Test size: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.9158\n",
      "  Validation F1: 0.9101\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.9140\n",
      "    Test F1 Score: 0.9159\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 5000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (10000, 500)\n",
      "  Valid size: (10000, 500)\n",
      "  Test size: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.9415\n",
      "  Validation F1: 0.9422\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.9531\n",
      "    Test F1 Score: 0.9536\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 100 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (200, 500)\n",
      "  Valid size: (200, 500)\n",
      "  Test size: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.9702\n",
      "  Validation F1: 0.9749\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.9500\n",
      "    Test F1 Score: 0.9510\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 1000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (2000, 500)\n",
      "  Valid size: (2000, 500)\n",
      "  Test size: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.9624\n",
      "  Validation F1: 0.9711\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.9755\n",
      "    Test F1 Score: 0.9758\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 5000 examples\n",
      "============================================================\n",
      "Loading datasets...\n",
      "  Train size: (10000, 500)\n",
      "  Valid size: (10000, 500)\n",
      "  Test size: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best cross-validation F1: 0.9827\n",
      "  Validation F1: 0.9816\n",
      "  Training final model on combined train+valid set...\n",
      "  Evaluating on test set...\n",
      "\n",
      "  Results:\n",
      "    Test Accuracy: 0.9892\n",
      "    Test F1 Score: 0.9892\n",
      "\n",
      "Results saved to dt_results.json\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF ALL EXPERIMENTS\n",
      "================================================================================\n",
      "Clauses    Examples   Features   Test Acc     Test F1     \n",
      "--------------------------------------------------------------------------------\n",
      "300        100        500        0.5350       0.5507      \n",
      "300        1000       500        0.6610       0.6558      \n",
      "300        5000       500        0.7793       0.7874      \n",
      "500        100        500        0.5200       0.5102      \n",
      "500        1000       500        0.6760       0.6667      \n",
      "500        5000       500        0.7883       0.7981      \n",
      "1000       100        500        0.7450       0.7437      \n",
      "1000       1000       500        0.7945       0.7966      \n",
      "1000       5000       500        0.8597       0.8646      \n",
      "1500       100        500        0.8450       0.8442      \n",
      "1500       1000       500        0.9140       0.9159      \n",
      "1500       5000       500        0.9531       0.9536      \n",
      "1800       100        500        0.9500       0.9510      \n",
      "1800       1000       500        0.9755       0.9758      \n",
      "1800       5000       500        0.9892       0.9892      \n",
      "================================================================================\n",
      "\n",
      "BEST HYPERPARAMETERS FOR EACH CONFIGURATION:\n",
      "================================================================================\n",
      "\n",
      "Clauses: 300, Examples: 100\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: log2\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 10\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 300, Examples: 1000\n",
      "  criterion: entropy\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 10\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 300, Examples: 5000\n",
      "  criterion: entropy\n",
      "  max_depth: 10\n",
      "  max_features: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 10\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 500, Examples: 100\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 500, Examples: 1000\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 2\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 500, Examples: 5000\n",
      "  criterion: entropy\n",
      "  max_depth: 10\n",
      "  max_features: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 2\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 1000, Examples: 100\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 5\n",
      "  splitter: best\n",
      "\n",
      "Clauses: 1000, Examples: 1000\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 10\n",
      "  splitter: best\n",
      "\n",
      "Clauses: 1000, Examples: 5000\n",
      "  criterion: entropy\n",
      "  max_depth: 10\n",
      "  max_features: None\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 5\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 1500, Examples: 100\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 5\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 1500, Examples: 1000\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 5\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 1500, Examples: 5000\n",
      "  criterion: gini\n",
      "  max_depth: 10\n",
      "  max_features: None\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 5\n",
      "  splitter: random\n",
      "\n",
      "Clauses: 1800, Examples: 100\n",
      "  criterion: entropy\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 2\n",
      "  splitter: best\n",
      "\n",
      "Clauses: 1800, Examples: 1000\n",
      "  criterion: entropy\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 5\n",
      "  splitter: best\n",
      "\n",
      "Clauses: 1800, Examples: 5000\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 10\n",
      "  splitter: random\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class CNFDatasetExperiment:\n",
    "    \"\"\"\n",
    "    Experiment runner for Decision Tree classification on CNF datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./all_data'):\n",
    "        \"\"\"\n",
    "        Initialize the experiment runner.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing the CSV datasets\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.results = []\n",
    "        \n",
    "        # Define hyperparameter grid for tuning\n",
    "        self.param_grid = {\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [None, 10, 20, 30, 50, 100],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': [None, 'sqrt', 'log2']\n",
    "        }\n",
    "    \n",
    "    def load_dataset(self, clauses, examples):\n",
    "        \"\"\"\n",
    "        Load train, validation, and test sets for a specific configuration.\n",
    "        \n",
    "        Args:\n",
    "            clauses: Number of clauses (300, 500, 1000, 1500, 1800)\n",
    "            examples: Number of examples (100, 1000, 5000)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with train, valid, and test DataFrames\n",
    "        \"\"\"\n",
    "        base_name = f\"c{clauses}_d{examples}\"\n",
    "        \n",
    "        train_file = self.data_dir / f\"train_{base_name}.csv\"\n",
    "        valid_file = self.data_dir / f\"valid_{base_name}.csv\"\n",
    "        test_file = self.data_dir / f\"test_{base_name}.csv\"\n",
    "        \n",
    "        # Check if files exist\n",
    "        for file in [train_file, valid_file, test_file]:\n",
    "            if not file.exists():\n",
    "                raise FileNotFoundError(f\"Dataset file not found: {file}\")\n",
    "        \n",
    "        # Load datasets\n",
    "        train_df = pd.read_csv(train_file, header=None)\n",
    "        valid_df = pd.read_csv(valid_file, header=None)\n",
    "        test_df = pd.read_csv(test_file, header=None)\n",
    "        \n",
    "        return {\n",
    "            'train': train_df,\n",
    "            'valid': valid_df,\n",
    "            'test': test_df\n",
    "        }\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"\n",
    "        Split features and labels from a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with features and label in last column\n",
    "            \n",
    "        Returns:\n",
    "            X (features), y (labels)\n",
    "        \"\"\"\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        return X, y\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Tune hyperparameters using validation set.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training labels\n",
    "            X_valid: Validation features\n",
    "            y_valid: Validation labels\n",
    "            \n",
    "        Returns:\n",
    "            Best parameters and best validation score\n",
    "        \"\"\"\n",
    "        print(\"  Tuning hyperparameters...\")\n",
    "        \n",
    "        # Use GridSearchCV with custom scoring\n",
    "        dt = DecisionTreeClassifier(random_state=42)\n",
    "        \n",
    "        # Perform grid search with cross-validation on training set\n",
    "        grid_search = GridSearchCV(\n",
    "            dt,\n",
    "            self.param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate best model on validation set\n",
    "        best_model = grid_search.best_estimator_\n",
    "        valid_score = f1_score(y_valid, best_model.predict(X_valid))\n",
    "        \n",
    "        print(f\"  Best cross-validation F1: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  Validation F1: {valid_score:.4f}\")\n",
    "        \n",
    "        return grid_search.best_params_, valid_score\n",
    "    \n",
    "    def train_final_model(self, X_train, y_train, X_valid, y_valid, best_params):\n",
    "        \"\"\"\n",
    "        Train final model on combined train+validation set.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training labels\n",
    "            X_valid: Validation features\n",
    "            y_valid: Validation labels\n",
    "            best_params: Best hyperparameters from tuning\n",
    "            \n",
    "        Returns:\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        print(\"  Training final model on combined train+valid set...\")\n",
    "        \n",
    "        # Combine training and validation sets\n",
    "        X_combined = np.vstack([X_train, X_valid])\n",
    "        y_combined = np.hstack([y_train, y_valid])\n",
    "        \n",
    "        # Train final model\n",
    "        final_model = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "        final_model.fit(X_combined, y_combined)\n",
    "        \n",
    "        return final_model\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate model on test set.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            X_test: Test features\n",
    "            y_test: Test labels\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with accuracy and F1 score\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    def run_experiment(self, clauses, examples):\n",
    "        \"\"\"\n",
    "        Run complete experiment for one dataset configuration.\n",
    "        \n",
    "        Args:\n",
    "            clauses: Number of clauses\n",
    "            examples: Number of examples\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Experiment: {clauses} clauses, {examples} examples\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load data\n",
    "        print(\"Loading datasets...\")\n",
    "        data = self.load_dataset(clauses, examples)\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, y_train = self.prepare_data(data['train'])\n",
    "        X_valid, y_valid = self.prepare_data(data['valid'])\n",
    "        X_test, y_test = self.prepare_data(data['test'])\n",
    "        \n",
    "        print(f\"  Train size: {X_train.shape}\")\n",
    "        print(f\"  Valid size: {X_valid.shape}\")\n",
    "        print(f\"  Test size: {X_test.shape}\")\n",
    "        \n",
    "        # Tune hyperparameters\n",
    "        best_params, valid_score = self.tune_hyperparameters(\n",
    "            X_train, y_train, X_valid, y_valid\n",
    "        )\n",
    "        \n",
    "        # Train final model\n",
    "        final_model = self.train_final_model(\n",
    "            X_train, y_train, X_valid, y_valid, best_params\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"  Evaluating on test set...\")\n",
    "        test_metrics = self.evaluate_model(final_model, X_test, y_test)\n",
    "        \n",
    "        # Compile results\n",
    "        result = {\n",
    "            'clauses': clauses,\n",
    "            'examples': examples,\n",
    "            'n_features': X_train.shape[1],\n",
    "            'best_params': best_params,\n",
    "            'validation_f1': valid_score,\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_f1_score': test_metrics['f1_score']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n  Results:\")\n",
    "        print(f\"    Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"    Test F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def discover_datasets(self):\n",
    "        \"\"\"\n",
    "        Discover available datasets in the data directory.\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples (clauses, examples) for available datasets\n",
    "        \"\"\"\n",
    "        available_datasets = []\n",
    "        \n",
    "        # Check all possible combinations\n",
    "        clause_configs = [300, 500, 1000, 1500, 1800]\n",
    "        example_configs = [100, 1000, 5000]\n",
    "        \n",
    "        print(\"\\nScanning for available datasets...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for clauses in clause_configs:\n",
    "            for examples in example_configs:\n",
    "                base_name = f\"c{clauses}_d{examples}\"\n",
    "                train_file = self.data_dir / f\"train_{base_name}.csv\"\n",
    "                valid_file = self.data_dir / f\"valid_{base_name}.csv\"\n",
    "                test_file = self.data_dir / f\"test_{base_name}.csv\"\n",
    "                \n",
    "                # Check if all three files exist\n",
    "                if train_file.exists() and valid_file.exists() and test_file.exists():\n",
    "                    available_datasets.append((clauses, examples))\n",
    "                    print(f\"✓ Found: {clauses} clauses, {examples} examples\")\n",
    "                else:\n",
    "                    missing = []\n",
    "                    if not train_file.exists():\n",
    "                        missing.append(\"train\")\n",
    "                    if not valid_file.exists():\n",
    "                        missing.append(\"valid\")\n",
    "                    if not test_file.exists():\n",
    "                        missing.append(\"test\")\n",
    "                    print(f\"✗ Missing ({clauses} clauses, {examples} examples): {', '.join(missing)}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Total available datasets: {len(available_datasets)}/15\\n\")\n",
    "        \n",
    "        return available_datasets\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        \"\"\"\n",
    "        Run experiments on all dataset configurations.\n",
    "        \"\"\"\n",
    "        print(\"Starting Decision Tree Experiments on CNF Datasets\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Data directory: {self.data_dir.absolute()}\")\n",
    "        \n",
    "        # Discover available datasets\n",
    "        available_datasets = self.discover_datasets()\n",
    "        \n",
    "        if not available_datasets:\n",
    "            print(\"\\nERROR: No complete datasets found!\")\n",
    "            print(f\"Please check that CSV files are in: {self.data_dir.absolute()}\")\n",
    "            print(\"Expected filename format: train_c[clauses]_d[examples].csv\")\n",
    "            return\n",
    "        \n",
    "        # Run experiments on available datasets\n",
    "        for clauses, examples in available_datasets:\n",
    "            try:\n",
    "                result = self.run_experiment(clauses, examples)\n",
    "                self.results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in experiment ({clauses} clauses, {examples} examples): {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        if self.results:\n",
    "            self.save_results()\n",
    "            self.print_summary()\n",
    "        else:\n",
    "            print(\"\\nNo experiments completed successfully.\")\n",
    "    \n",
    "    def save_results(self, filename='dt_results.json'):\n",
    "        \"\"\"Save results to JSON file.\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary table of all results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF ALL EXPERIMENTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Clauses':<10} {'Examples':<10} {'Features':<10} {'Test Acc':<12} {'Test F1':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for result in self.results:\n",
    "            print(f\"{result['clauses']:<10} {result['examples']:<10} \"\n",
    "                  f\"{result['n_features']:<10} \"\n",
    "                  f\"{result['test_accuracy']:<12.4f} \"\n",
    "                  f\"{result['test_f1_score']:<12.4f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Print best parameters for each configuration\n",
    "        print(\"\\nBEST HYPERPARAMETERS FOR EACH CONFIGURATION:\")\n",
    "        print(\"=\"*80)\n",
    "        for result in self.results:\n",
    "            print(f\"\\nClauses: {result['clauses']}, Examples: {result['examples']}\")\n",
    "            for param, value in result['best_params'].items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    # Initialize experiment runner\n",
    "    # Update 'data_dir' to point to your all_data folder\n",
    "    experiment = CNFDatasetExperiment(data_dir='./all_data')\n",
    "    \n",
    "    # Run all experiments\n",
    "    experiment.run_all_experiments()\n",
    "    \n",
    "    # Results are automatically saved and printed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea74969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPERIMENT 2: BAGGING WITH DECISION TREES\n",
      "CNF Boolean Formula Classification\n",
      "============================================================\n",
      "\n",
      "BAGGING WITH DECISION TREES - CNF CLASSIFICATION\n",
      "============================================================\n",
      "Data directory: c:\\utd\\sem1\\machinelearning\\project2\\project2_data\\all_data\n",
      "Time limit: 10 minutes\n",
      "\n",
      "Scanning for available datasets...\n",
      "------------------------------------------------------------\n",
      "Found: 300 clauses, 100 examples\n",
      "Found: 300 clauses, 1000 examples\n",
      "Found: 300 clauses, 5000 examples\n",
      "Found: 500 clauses, 100 examples\n",
      "Found: 500 clauses, 1000 examples\n",
      "Found: 500 clauses, 5000 examples\n",
      "Found: 1000 clauses, 100 examples\n",
      "Found: 1000 clauses, 1000 examples\n",
      "Found: 1000 clauses, 5000 examples\n",
      "Found: 1500 clauses, 100 examples\n",
      "Found: 1500 clauses, 1000 examples\n",
      "Found: 1500 clauses, 5000 examples\n",
      "Found: 1800 clauses, 100 examples\n",
      "Found: 1800 clauses, 1000 examples\n",
      "Found: 1800 clauses, 5000 examples\n",
      "------------------------------------------------------------\n",
      "Total available datasets: 15/15\n",
      "\n",
      "Processing 15 datasets\n",
      "\n",
      "\n",
      "[Dataset 1/15] Elapsed: 0.0m, Remaining: 10.0m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.7438, Validation F1: 0.6765\n",
      "  Tuning time: 9.1s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.8000\n",
      "  Test F1 Score: 0.8020\n",
      "\n",
      "[Dataset 2/15] Elapsed: 0.2m, Remaining: 9.8m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.8426, Validation F1: 0.8850\n",
      "  Tuning time: 10.3s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.8855\n",
      "  Test F1 Score: 0.8902\n",
      "\n",
      "[Dataset 3/15] Elapsed: 0.4m, Remaining: 9.6m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.8878, Validation F1: 0.9163\n",
      "  Tuning time: 74.5s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9353\n",
      "  Test F1 Score: 0.9374\n",
      "\n",
      "[Dataset 4/15] Elapsed: 1.9m, Remaining: 8.1m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.7414, Validation F1: 0.7938\n",
      "  Tuning time: 2.1s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.8900\n",
      "  Test F1 Score: 0.8962\n",
      "\n",
      "[Dataset 5/15] Elapsed: 2.0m, Remaining: 8.0m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.8903, Validation F1: 0.8882\n",
      "  Tuning time: 14.3s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.8880\n",
      "  Test F1 Score: 0.8861\n",
      "\n",
      "[Dataset 6/15] Elapsed: 2.3m, Remaining: 7.7m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9187, Validation F1: 0.9287\n",
      "  Tuning time: 83.1s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9505\n",
      "  Test F1 Score: 0.9508\n",
      "\n",
      "[Dataset 7/15] Elapsed: 3.9m, Remaining: 6.1m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9108, Validation F1: 0.8750\n",
      "  Tuning time: 1.3s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9100\n",
      "  Test F1 Score: 0.9082\n",
      "\n",
      "[Dataset 8/15] Elapsed: 4.0m, Remaining: 6.0m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9445, Validation F1: 0.9610\n",
      "  Tuning time: 9.1s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9495\n",
      "  Test F1 Score: 0.9503\n",
      "\n",
      "[Dataset 9/15] Elapsed: 4.2m, Remaining: 5.8m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9623, Validation F1: 0.9724\n",
      "  Tuning time: 67.7s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9784\n",
      "  Test F1 Score: 0.9784\n",
      "\n",
      "[Dataset 10/15] Elapsed: 5.5m, Remaining: 4.5m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9546, Validation F1: 0.9804\n",
      "  Tuning time: 1.2s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9800\n",
      "  Test F1 Score: 0.9804\n",
      "\n",
      "[Dataset 11/15] Elapsed: 5.6m, Remaining: 4.4m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9890, Validation F1: 0.9889\n",
      "  Tuning time: 8.2s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9885\n",
      "  Test F1 Score: 0.9885\n",
      "\n",
      "[Dataset 12/15] Elapsed: 5.7m, Remaining: 4.3m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9930, Validation F1: 0.9935\n",
      "  Tuning time: 73.6s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9941\n",
      "  Test F1 Score: 0.9941\n",
      "\n",
      "[Dataset 13/15] Elapsed: 7.1m, Remaining: 2.9m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 1.0000, Validation F1: 1.0000\n",
      "  Tuning time: 1.3s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9850\n",
      "  Test F1 Score: 0.9849\n",
      "\n",
      "[Dataset 14/15] Elapsed: 7.2m, Remaining: 2.8m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9970, Validation F1: 0.9925\n",
      "  Tuning time: 8.2s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9950\n",
      "  Test F1 Score: 0.9950\n",
      "\n",
      "[Dataset 15/15] Elapsed: 7.3m, Remaining: 2.7m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9969, Validation F1: 0.9987\n",
      "  Tuning time: 70.3s\n",
      "  Training final model...\n",
      "  Evaluating on test set...\n",
      "  Test Accuracy: 0.9988\n",
      "  Test F1 Score: 0.9988\n",
      "\n",
      "============================================================\n",
      "Total execution time: 8.55 minutes\n",
      "============================================================\n",
      "\n",
      "Results saved to bagging_results.json\n",
      "\n",
      "================================================================================\n",
      "SUMMARY - BAGGING WITH DECISION TREES\n",
      "================================================================================\n",
      "Clauses    Examples   Features   Test Acc     Test F1     \n",
      "--------------------------------------------------------------------------------\n",
      "300        100        500        0.8000       0.8020      \n",
      "300        1000       500        0.8855       0.8902      \n",
      "300        5000       500        0.9353       0.9374      \n",
      "500        100        500        0.8900       0.8962      \n",
      "500        1000       500        0.8880       0.8861      \n",
      "500        5000       500        0.9505       0.9508      \n",
      "1000       100        500        0.9100       0.9082      \n",
      "1000       1000       500        0.9495       0.9503      \n",
      "1000       5000       500        0.9784       0.9784      \n",
      "1500       100        500        0.9800       0.9804      \n",
      "1500       1000       500        0.9885       0.9885      \n",
      "1500       5000       500        0.9941       0.9941      \n",
      "1800       100        500        0.9850       0.9849      \n",
      "1800       1000       500        0.9950       0.9950      \n",
      "1800       5000       500        0.9988       0.9988      \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "BEST HYPERPARAMETERS FOR EACH CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Configuration: 300 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 1.0\n",
      "  max_samples: 0.8\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: entropy\n",
      "  max_depth: 20\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.6765\n",
      "  Test Accuracy: 0.8000\n",
      "  Test F1 Score: 0.8020\n",
      "\n",
      "Configuration: 300 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 1.0\n",
      "  max_samples: 0.8\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: entropy\n",
      "  max_depth: 20\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.8850\n",
      "  Test Accuracy: 0.8855\n",
      "  Test F1 Score: 0.8902\n",
      "\n",
      "Configuration: 300 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.9163\n",
      "  Test Accuracy: 0.9353\n",
      "  Test F1 Score: 0.9374\n",
      "\n",
      "Configuration: 500 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.7938\n",
      "  Test Accuracy: 0.8900\n",
      "  Test F1 Score: 0.8962\n",
      "\n",
      "Configuration: 500 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 1.0\n",
      "  max_samples: 0.8\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: gini\n",
      "  max_depth: 20\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.8882\n",
      "  Test Accuracy: 0.8880\n",
      "  Test F1 Score: 0.8861\n",
      "\n",
      "Configuration: 500 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.9287\n",
      "  Test Accuracy: 0.9505\n",
      "  Test F1 Score: 0.9508\n",
      "\n",
      "Configuration: 1000 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.8750\n",
      "  Test Accuracy: 0.9100\n",
      "  Test F1 Score: 0.9082\n",
      "\n",
      "Configuration: 1000 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 0.8\n",
      "  n_estimators: 50\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: entropy\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 10\n",
      "Performance:\n",
      "  Validation F1: 0.9610\n",
      "  Test Accuracy: 0.9495\n",
      "  Test F1 Score: 0.9503\n",
      "\n",
      "Configuration: 1000 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.9724\n",
      "  Test Accuracy: 0.9784\n",
      "  Test F1 Score: 0.9784\n",
      "\n",
      "Configuration: 1500 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 50\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: entropy\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.9804\n",
      "  Test Accuracy: 0.9800\n",
      "  Test F1 Score: 0.9804\n",
      "\n",
      "Configuration: 1500 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 0.8\n",
      "  n_estimators: 50\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: entropy\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 10\n",
      "Performance:\n",
      "  Validation F1: 0.9889\n",
      "  Test Accuracy: 0.9885\n",
      "  Test F1 Score: 0.9885\n",
      "\n",
      "Configuration: 1500 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.9935\n",
      "  Test Accuracy: 0.9941\n",
      "  Test F1 Score: 0.9941\n",
      "\n",
      "Configuration: 1800 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 100\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: gini\n",
      "  max_depth: 20\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 1.0000\n",
      "  Test Accuracy: 0.9850\n",
      "  Test F1 Score: 0.9849\n",
      "\n",
      "Configuration: 1800 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 1.0\n",
      "  n_estimators: 25\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: entropy\n",
      "  max_depth: 20\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.9925\n",
      "  Test Accuracy: 0.9950\n",
      "  Test F1 Score: 0.9950\n",
      "\n",
      "Configuration: 1800 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Bagging Parameters:\n",
      "  bootstrap: True\n",
      "  bootstrap_features: False\n",
      "  max_features: 0.8\n",
      "  max_samples: 0.8\n",
      "  n_estimators: 25\n",
      "Base Estimator (Decision Tree) Parameters:\n",
      "  criterion: entropy\n",
      "  max_depth: 20\n",
      "  min_samples_leaf: 4\n",
      "  min_samples_split: 2\n",
      "Performance:\n",
      "  Validation F1: 0.9987\n",
      "  Test Accuracy: 0.9988\n",
      "  Test F1 Score: 0.9988\n",
      "\n",
      "EXPERIMENT COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "class BaggingCNFExperiment:\n",
    "    \"\"\"\n",
    "    Experiment runner for Bagging with Decision Tree classification on CNF datasets.\n",
    "    Optimized to run under 10 minutes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./all_data'):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.results = []\n",
    "        \n",
    "        # Highly optimized hyperparameter distributions\n",
    "        self.param_distributions = {\n",
    "            # Bagging-specific parameters\n",
    "            'n_estimators': [25, 50, 100],\n",
    "            'max_samples': [0.8, 1.0],\n",
    "            'max_features': [0.8, 1.0],\n",
    "            'bootstrap': [True],\n",
    "            'bootstrap_features': [False],\n",
    "            \n",
    "            # Base estimator (Decision Tree) parameters  \n",
    "            'estimator__criterion': ['gini', 'entropy'],\n",
    "            'estimator__max_depth': [None, 20, 40],\n",
    "            'estimator__min_samples_split': [2, 10],\n",
    "            'estimator__min_samples_leaf': [1, 4],\n",
    "        }\n",
    "        \n",
    "        # Reduced iterations for speed\n",
    "        self.n_iter = 20  # Reduced from 50\n",
    "    \n",
    "    def discover_datasets(self):\n",
    "        available_datasets = []\n",
    "        clause_configs = [300, 500, 1000, 1500, 1800]\n",
    "        example_configs = [100, 1000, 5000]\n",
    "        \n",
    "        print(\"\\nScanning for available datasets...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for clauses in clause_configs:\n",
    "            for examples in example_configs:\n",
    "                base_name = f\"c{clauses}_d{examples}\"\n",
    "                train_file = self.data_dir / f\"train_{base_name}.csv\"\n",
    "                valid_file = self.data_dir / f\"valid_{base_name}.csv\"\n",
    "                test_file = self.data_dir / f\"test_{base_name}.csv\"\n",
    "                \n",
    "                if train_file.exists() and valid_file.exists() and test_file.exists():\n",
    "                    available_datasets.append((clauses, examples))\n",
    "                    print(f\"Found: {clauses} clauses, {examples} examples\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Total available datasets: {len(available_datasets)}/15\\n\")\n",
    "        \n",
    "        return available_datasets\n",
    "    \n",
    "    def load_dataset(self, clauses, examples):\n",
    "        base_name = f\"c{clauses}_d{examples}\"\n",
    "        \n",
    "        train_file = self.data_dir / f\"train_{base_name}.csv\"\n",
    "        valid_file = self.data_dir / f\"valid_{base_name}.csv\"\n",
    "        test_file = self.data_dir / f\"test_{base_name}.csv\"\n",
    "        \n",
    "        train_df = pd.read_csv(train_file, header=None)\n",
    "        valid_df = pd.read_csv(valid_file, header=None)\n",
    "        test_df = pd.read_csv(test_file, header=None)\n",
    "        \n",
    "        return {\n",
    "            'train': train_df,\n",
    "            'valid': valid_df,\n",
    "            'test': test_df\n",
    "        }\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        return X, y\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_valid, y_valid):\n",
    "        print(\"  Tuning hyperparameters...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        base_estimator = DecisionTreeClassifier(random_state=42)\n",
    "        bagging = BaggingClassifier(\n",
    "            estimator=base_estimator,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Using 2-fold CV for maximum speed\n",
    "        random_search = RandomizedSearchCV(\n",
    "            bagging,\n",
    "            self.param_distributions,\n",
    "            n_iter=self.n_iter,\n",
    "            cv=2,  # Reduced from 3 to 2\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = random_search.best_estimator_\n",
    "        valid_score = f1_score(y_valid, best_model.predict(X_valid))\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"  Best CV F1: {random_search.best_score_:.4f}, Validation F1: {valid_score:.4f}\")\n",
    "        print(f\"  Tuning time: {elapsed_time:.1f}s\")\n",
    "        \n",
    "        return random_search.best_params_, valid_score\n",
    "    \n",
    "    def train_final_model(self, X_train, y_train, X_valid, y_valid, best_params):\n",
    "        print(\"  Training final model...\")\n",
    "        \n",
    "        X_combined = np.vstack([X_train, X_valid])\n",
    "        y_combined = np.hstack([y_train, y_valid])\n",
    "        \n",
    "        bagging_params = {}\n",
    "        estimator_params = {}\n",
    "        \n",
    "        for key, value in best_params.items():\n",
    "            if key.startswith('estimator__'):\n",
    "                estimator_params[key.replace('estimator__', '')] = value\n",
    "            else:\n",
    "                bagging_params[key] = value\n",
    "        \n",
    "        base_estimator = DecisionTreeClassifier(**estimator_params, random_state=42)\n",
    "        final_model = BaggingClassifier(\n",
    "            estimator=base_estimator,\n",
    "            **bagging_params,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        final_model.fit(X_combined, y_combined)\n",
    "        \n",
    "        return final_model\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    def run_experiment(self, clauses, examples):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Experiment: {clauses} clauses, {examples} examples\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        data = self.load_dataset(clauses, examples)\n",
    "        \n",
    "        X_train, y_train = self.prepare_data(data['train'])\n",
    "        X_valid, y_valid = self.prepare_data(data['valid'])\n",
    "        X_test, y_test = self.prepare_data(data['test'])\n",
    "        \n",
    "        print(f\"  Train: {X_train.shape}, Valid: {X_valid.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        best_params, valid_score = self.tune_hyperparameters(\n",
    "            X_train, y_train, X_valid, y_valid\n",
    "        )\n",
    "        \n",
    "        final_model = self.train_final_model(\n",
    "            X_train, y_train, X_valid, y_valid, best_params\n",
    "        )\n",
    "        \n",
    "        print(\"  Evaluating on test set...\")\n",
    "        test_metrics = self.evaluate_model(final_model, X_test, y_test)\n",
    "        \n",
    "        result = {\n",
    "            'clauses': clauses,\n",
    "            'examples': examples,\n",
    "            'n_features': X_train.shape[1],\n",
    "            'best_params': best_params,\n",
    "            'validation_f1': valid_score,\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_f1_score': test_metrics['f1_score']\n",
    "        }\n",
    "        \n",
    "        print(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Test F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        print(\"\\nBAGGING WITH DECISION TREES - CNF CLASSIFICATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Data directory: {self.data_dir.absolute()}\")\n",
    "        print(\"Time limit: 10 minutes\")\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        time_limit = 10 * 60\n",
    "        \n",
    "        available_datasets = self.discover_datasets()\n",
    "        \n",
    "        if not available_datasets:\n",
    "            print(\"\\nERROR: No complete datasets found!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Processing {len(available_datasets)} datasets\\n\")\n",
    "        \n",
    "        for idx, (clauses, examples) in enumerate(available_datasets, 1):\n",
    "            elapsed = time.time() - total_start_time\n",
    "            remaining = time_limit - elapsed\n",
    "            \n",
    "            print(f\"\\n[Dataset {idx}/{len(available_datasets)}] Elapsed: {elapsed/60:.1f}m, Remaining: {remaining/60:.1f}m\")\n",
    "            \n",
    "            if remaining < 30:\n",
    "                print(\"Warning: Less than 30 seconds remaining, stopping.\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                result = self.run_experiment(clauses, examples)\n",
    "                self.results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Total execution time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if self.results:\n",
    "            self.save_results()\n",
    "            self.print_summary()\n",
    "        else:\n",
    "            print(\"No experiments completed.\")\n",
    "    \n",
    "    def save_results(self, filename='bagging_results.json'):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY - BAGGING WITH DECISION TREES\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Clauses':<10} {'Examples':<10} {'Features':<10} {'Test Acc':<12} {'Test F1':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for result in self.results:\n",
    "            print(f\"{result['clauses']:<10} {result['examples']:<10} \"\n",
    "                  f\"{result['n_features']:<10} \"\n",
    "                  f\"{result['test_accuracy']:<12.4f} \"\n",
    "                  f\"{result['test_f1_score']:<12.4f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BEST HYPERPARAMETERS FOR EACH CONFIGURATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for result in self.results:\n",
    "            print(f\"\\nConfiguration: {result['clauses']} clauses, {result['examples']} examples\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            bagging_params = {}\n",
    "            estimator_params = {}\n",
    "            \n",
    "            for param, value in result['best_params'].items():\n",
    "                if param.startswith('estimator__'):\n",
    "                    estimator_params[param] = value\n",
    "                else:\n",
    "                    bagging_params[param] = value\n",
    "            \n",
    "            print(\"Bagging Parameters:\")\n",
    "            for param, value in sorted(bagging_params.items()):\n",
    "                print(f\"  {param}: {value}\")\n",
    "            \n",
    "            print(\"Base Estimator (Decision Tree) Parameters:\")\n",
    "            for param, value in sorted(estimator_params.items()):\n",
    "                param_clean = param.replace('estimator__', '')\n",
    "                print(f\"  {param_clean}: {value}\")\n",
    "            \n",
    "            print(f\"Performance:\")\n",
    "            print(f\"  Validation F1: {result['validation_f1']:.4f}\")\n",
    "            print(f\"  Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "            print(f\"  Test F1 Score: {result['test_f1_score']:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"\\nEXPERIMENT 2: BAGGING WITH DECISION TREES\")\n",
    "    print(\"CNF Boolean Formula Classification\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    experiment = BaggingCNFExperiment(data_dir='./all_data')\n",
    "    experiment.run_all_experiments()\n",
    "    \n",
    "    print(\"\\nEXPERIMENT COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc73d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPERIMENT 3: RANDOM FOREST CLASSIFIER\n",
      "CNF Boolean Formula Classification\n",
      "============================================================\n",
      "\n",
      "RANDOM FOREST CLASSIFIER - CNF CLASSIFICATION\n",
      "============================================================\n",
      "Data directory: c:\\utd\\sem1\\machinelearning\\project2\\project2_data\\all_data\n",
      "Focus: Best accuracy and F1 scores\n",
      "\n",
      "Scanning for available datasets...\n",
      "------------------------------------------------------------\n",
      "Found: 300 clauses, 100 examples\n",
      "Found: 300 clauses, 1000 examples\n",
      "Found: 300 clauses, 5000 examples\n",
      "Found: 500 clauses, 100 examples\n",
      "Found: 500 clauses, 1000 examples\n",
      "Found: 500 clauses, 5000 examples\n",
      "Found: 1000 clauses, 100 examples\n",
      "Found: 1000 clauses, 1000 examples\n",
      "Found: 1000 clauses, 5000 examples\n",
      "Found: 1500 clauses, 100 examples\n",
      "Found: 1500 clauses, 1000 examples\n",
      "Found: 1500 clauses, 5000 examples\n",
      "Found: 1800 clauses, 100 examples\n",
      "Found: 1800 clauses, 1000 examples\n",
      "Found: 1800 clauses, 5000 examples\n",
      "------------------------------------------------------------\n",
      "Total available datasets: 15/15\n",
      "\n",
      "Processing 15 datasets\n",
      "\n",
      "\n",
      "[Dataset 1/15] Elapsed: 0.0m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.8492\n",
      "  Train F1: 1.0000, Validation F1: 0.7795\n",
      "  Validation Accuracy: 0.7850\n",
      "  Tuning time: 15.8s\n",
      "  Note: Possible overfitting detected (train-valid gap: 0.2205)\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.8450\n",
      "    F1 Score:  0.8473\n",
      "    Precision: 0.8350\n",
      "    Recall:    0.8600\n",
      "  Dataset completed in 16.3s\n",
      "\n",
      "[Dataset 2/15] Elapsed: 0.3m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.8910\n",
      "  Train F1: 1.0000, Validation F1: 0.9024\n",
      "  Validation Accuracy: 0.8995\n",
      "  Tuning time: 21.0s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.8925\n",
      "    F1 Score:  0.8942\n",
      "    Precision: 0.8800\n",
      "    Recall:    0.9090\n",
      "  Dataset completed in 22.0s\n",
      "\n",
      "[Dataset 3/15] Elapsed: 0.6m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.9229\n",
      "  Train F1: 1.0000, Validation F1: 0.9308\n",
      "  Validation Accuracy: 0.9297\n",
      "  Tuning time: 38.3s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9305\n",
      "    F1 Score:  0.9317\n",
      "    Precision: 0.9153\n",
      "    Recall:    0.9488\n",
      "  Dataset completed in 40.2s\n",
      "\n",
      "[Dataset 4/15] Elapsed: 1.3m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.8897\n",
      "  Train F1: 1.0000, Validation F1: 0.9118\n",
      "  Validation Accuracy: 0.9100\n",
      "  Tuning time: 10.8s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9000\n",
      "    F1 Score:  0.8958\n",
      "    Precision: 0.9348\n",
      "    Recall:    0.8600\n",
      "  Dataset completed in 11.3s\n",
      "\n",
      "[Dataset 5/15] Elapsed: 1.5m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.9502\n",
      "  Train F1: 0.9995, Validation F1: 0.9498\n",
      "  Validation Accuracy: 0.9495\n",
      "  Tuning time: 20.8s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9520\n",
      "    F1 Score:  0.9520\n",
      "    Precision: 0.9511\n",
      "    Recall:    0.9530\n",
      "  Dataset completed in 21.4s\n",
      "\n",
      "[Dataset 6/15] Elapsed: 1.9m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.9690\n",
      "  Train F1: 0.9998, Validation F1: 0.9675\n",
      "  Validation Accuracy: 0.9672\n",
      "  Tuning time: 37.7s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9700\n",
      "    F1 Score:  0.9703\n",
      "    Precision: 0.9602\n",
      "    Recall:    0.9806\n",
      "  Dataset completed in 40.0s\n",
      "\n",
      "[Dataset 7/15] Elapsed: 2.5m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.9951\n",
      "  Train F1: 1.0000, Validation F1: 1.0000\n",
      "  Validation Accuracy: 1.0000\n",
      "  Tuning time: 11.0s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  Dataset completed in 11.4s\n",
      "\n",
      "[Dataset 8/15] Elapsed: 2.7m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.9985\n",
      "  Train F1: 1.0000, Validation F1: 0.9980\n",
      "  Validation Accuracy: 0.9980\n",
      "  Tuning time: 20.0s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9975\n",
      "    F1 Score:  0.9975\n",
      "    Precision: 0.9980\n",
      "    Recall:    0.9970\n",
      "  Dataset completed in 20.9s\n",
      "\n",
      "[Dataset 9/15] Elapsed: 3.1m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 0.9981\n",
      "  Train F1: 1.0000, Validation F1: 0.9983\n",
      "  Validation Accuracy: 0.9983\n",
      "  Tuning time: 37.9s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9980\n",
      "    F1 Score:  0.9980\n",
      "    Precision: 0.9982\n",
      "    Recall:    0.9978\n",
      "  Dataset completed in 40.3s\n",
      "\n",
      "[Dataset 10/15] Elapsed: 3.7m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 1.0000\n",
      "  Train F1: 1.0000, Validation F1: 1.0000\n",
      "  Validation Accuracy: 1.0000\n",
      "  Tuning time: 10.6s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  Dataset completed in 10.9s\n",
      "\n",
      "[Dataset 11/15] Elapsed: 3.9m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 1.0000\n",
      "  Train F1: 1.0000, Validation F1: 1.0000\n",
      "  Validation Accuracy: 1.0000\n",
      "  Tuning time: 19.0s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  Dataset completed in 19.7s\n",
      "\n",
      "[Dataset 12/15] Elapsed: 4.2m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 1.0000\n",
      "  Train F1: 1.0000, Validation F1: 0.9999\n",
      "  Validation Accuracy: 0.9999\n",
      "  Tuning time: 36.9s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  Dataset completed in 39.3s\n",
      "\n",
      "[Dataset 13/15] Elapsed: 4.9m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 100 examples\n",
      "============================================================\n",
      "  Train: (200, 500), Valid: (200, 500), Test: (200, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 1.0000\n",
      "  Train F1: 1.0000, Validation F1: 1.0000\n",
      "  Validation Accuracy: 1.0000\n",
      "  Tuning time: 11.2s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  Dataset completed in 11.5s\n",
      "\n",
      "[Dataset 14/15] Elapsed: 5.1m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 1000 examples\n",
      "============================================================\n",
      "  Train: (2000, 500), Valid: (2000, 500), Test: (2000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 1.0000\n",
      "  Train F1: 1.0000, Validation F1: 1.0000\n",
      "  Validation Accuracy: 1.0000\n",
      "  Tuning time: 18.2s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  Dataset completed in 18.6s\n",
      "\n",
      "[Dataset 15/15] Elapsed: 5.4m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 5000 examples\n",
      "============================================================\n",
      "  Train: (10000, 500), Valid: (10000, 500), Test: (10000, 500)\n",
      "  Class balance - Train: 50.00% positive, Test: 50.00% positive\n",
      "  Tuning hyperparameters with extensive search...\n",
      "  Best CV F1: 1.0000\n",
      "  Train F1: 1.0000, Validation F1: 0.9999\n",
      "  Validation Accuracy: 0.9999\n",
      "  Tuning time: 40.2s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  Dataset completed in 42.2s\n",
      "\n",
      "============================================================\n",
      "Total execution time: 6.10 minutes\n",
      "Completed 15/15 datasets\n",
      "============================================================\n",
      "\n",
      "Results saved to random_forest_results.json\n",
      "\n",
      "==========================================================================================\n",
      "SUMMARY - RANDOM FOREST CLASSIFIER\n",
      "==========================================================================================\n",
      "Clauses    Examples   Features   Test Acc     Test F1      Precision    Recall      \n",
      "------------------------------------------------------------------------------------------\n",
      "300        100        500        0.8450       0.8473       0.8350       0.8600      \n",
      "300        1000       500        0.8925       0.8942       0.8800       0.9090      \n",
      "300        5000       500        0.9305       0.9317       0.9153       0.9488      \n",
      "500        100        500        0.9000       0.8958       0.9348       0.8600      \n",
      "500        1000       500        0.9520       0.9520       0.9511       0.9530      \n",
      "500        5000       500        0.9700       0.9703       0.9602       0.9806      \n",
      "1000       100        500        1.0000       1.0000       1.0000       1.0000      \n",
      "1000       1000       500        0.9975       0.9975       0.9980       0.9970      \n",
      "1000       5000       500        0.9980       0.9980       0.9982       0.9978      \n",
      "1500       100        500        1.0000       1.0000       1.0000       1.0000      \n",
      "1500       1000       500        1.0000       1.0000       1.0000       1.0000      \n",
      "1500       5000       500        1.0000       1.0000       1.0000       1.0000      \n",
      "1800       100        500        1.0000       1.0000       1.0000       1.0000      \n",
      "1800       1000       500        1.0000       1.0000       1.0000       1.0000      \n",
      "1800       5000       500        1.0000       1.0000       1.0000       1.0000      \n",
      "==========================================================================================\n",
      "\n",
      "================================================================================\n",
      "BEST HYPERPARAMETERS FOR EACH CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Configuration: 300 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: entropy\n",
      "  max_depth: None\n",
      "  max_features: log2\n",
      "  max_samples: 0.9\n",
      "  min_impurity_decrease: 0.0001\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 10\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.7795\n",
      "  Test Accuracy:  0.8450\n",
      "  Test F1 Score:  0.8473\n",
      "  Test Precision: 0.8350\n",
      "  Test Recall:    0.8600\n",
      "\n",
      "Configuration: 300 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: entropy\n",
      "  max_depth: 50\n",
      "  max_features: log2\n",
      "  max_samples: 1.0\n",
      "  min_impurity_decrease: 0.0\n",
      "  min_samples_leaf: 8\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9024\n",
      "  Test Accuracy:  0.8925\n",
      "  Test F1 Score:  0.8942\n",
      "  Test Precision: 0.8800\n",
      "  Test Recall:    0.9090\n",
      "\n",
      "Configuration: 300 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: None\n",
      "  criterion: entropy\n",
      "  max_depth: 30\n",
      "  max_features: log2\n",
      "  max_samples: 1.0\n",
      "  min_impurity_decrease: 0.0\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 15\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9308\n",
      "  Test Accuracy:  0.9305\n",
      "  Test F1 Score:  0.9317\n",
      "  Test Precision: 0.9153\n",
      "  Test Recall:    0.9488\n",
      "\n",
      "Configuration: 500 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: entropy\n",
      "  max_depth: 50\n",
      "  max_features: log2\n",
      "  max_samples: 1.0\n",
      "  min_impurity_decrease: 0.0\n",
      "  min_samples_leaf: 8\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9118\n",
      "  Test Accuracy:  0.9000\n",
      "  Test F1 Score:  0.8958\n",
      "  Test Precision: 0.9348\n",
      "  Test Recall:    0.8600\n",
      "\n",
      "Configuration: 500 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: None\n",
      "  criterion: gini\n",
      "  max_depth: 30\n",
      "  max_features: log2\n",
      "  max_samples: 1.0\n",
      "  min_impurity_decrease: 0.001\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9498\n",
      "  Test Accuracy:  0.9520\n",
      "  Test F1 Score:  0.9520\n",
      "  Test Precision: 0.9511\n",
      "  Test Recall:    0.9530\n",
      "\n",
      "Configuration: 500 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: entropy\n",
      "  max_depth: 50\n",
      "  max_features: log2\n",
      "  max_samples: 1.0\n",
      "  min_impurity_decrease: 0.0\n",
      "  min_samples_leaf: 8\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9675\n",
      "  Test Accuracy:  0.9700\n",
      "  Test F1 Score:  0.9703\n",
      "  Test Precision: 0.9602\n",
      "  Test Recall:    0.9806\n",
      "\n",
      "Configuration: 1000 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: None\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: log2\n",
      "  max_samples: 1.0\n",
      "  min_impurity_decrease: 0.001\n",
      "  min_samples_leaf: 8\n",
      "  min_samples_split: 10\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 1.0000\n",
      "  Test Accuracy:  1.0000\n",
      "  Test F1 Score:  1.0000\n",
      "  Test Precision: 1.0000\n",
      "  Test Recall:    1.0000\n",
      "\n",
      "Configuration: 1000 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: entropy\n",
      "  max_depth: None\n",
      "  max_features: log2\n",
      "  max_samples: 0.9\n",
      "  min_impurity_decrease: 0.0001\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 10\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9980\n",
      "  Test Accuracy:  0.9975\n",
      "  Test F1 Score:  0.9975\n",
      "  Test Precision: 0.9980\n",
      "  Test Recall:    0.9970\n",
      "\n",
      "Configuration: 1000 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: gini\n",
      "  max_depth: 70\n",
      "  max_features: log2\n",
      "  max_samples: 0.8\n",
      "  min_impurity_decrease: 0.0\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9983\n",
      "  Test Accuracy:  0.9980\n",
      "  Test F1 Score:  0.9980\n",
      "  Test Precision: 0.9982\n",
      "  Test Recall:    0.9978\n",
      "\n",
      "Configuration: 1500 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: None\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: log2\n",
      "  max_samples: 1.0\n",
      "  min_impurity_decrease: 0.001\n",
      "  min_samples_leaf: 8\n",
      "  min_samples_split: 10\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 1.0000\n",
      "  Test Accuracy:  1.0000\n",
      "  Test F1 Score:  1.0000\n",
      "  Test Precision: 1.0000\n",
      "  Test Recall:    1.0000\n",
      "\n",
      "Configuration: 1500 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: None\n",
      "  criterion: gini\n",
      "  max_depth: None\n",
      "  max_features: log2\n",
      "  max_samples: 1.0\n",
      "  min_impurity_decrease: 0.001\n",
      "  min_samples_leaf: 8\n",
      "  min_samples_split: 10\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 1.0000\n",
      "  Test Accuracy:  1.0000\n",
      "  Test F1 Score:  1.0000\n",
      "  Test Precision: 1.0000\n",
      "  Test Recall:    1.0000\n",
      "\n",
      "Configuration: 1500 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: gini\n",
      "  max_depth: 70\n",
      "  max_features: log2\n",
      "  max_samples: 0.8\n",
      "  min_impurity_decrease: 0.0\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9999\n",
      "  Test Accuracy:  1.0000\n",
      "  Test F1 Score:  1.0000\n",
      "  Test Precision: 1.0000\n",
      "  Test Recall:    1.0000\n",
      "\n",
      "Configuration: 1800 clauses, 100 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: gini\n",
      "  max_depth: 50\n",
      "  max_features: sqrt\n",
      "  max_samples: 0.9\n",
      "  min_impurity_decrease: 0.001\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 100\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 1.0000\n",
      "  Test Accuracy:  1.0000\n",
      "  Test F1 Score:  1.0000\n",
      "  Test Precision: 1.0000\n",
      "  Test Recall:    1.0000\n",
      "\n",
      "Configuration: 1800 clauses, 1000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  criterion: gini\n",
      "  max_depth: 50\n",
      "  max_features: sqrt\n",
      "  max_samples: 0.9\n",
      "  min_impurity_decrease: 0.001\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 100\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 1.0000\n",
      "  Test Accuracy:  1.0000\n",
      "  Test F1 Score:  1.0000\n",
      "  Test Precision: 1.0000\n",
      "  Test Recall:    1.0000\n",
      "\n",
      "Configuration: 1800 clauses, 5000 examples\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Parameters:\n",
      "  bootstrap: True\n",
      "  class_weight: None\n",
      "  criterion: entropy\n",
      "  max_depth: 50\n",
      "  max_features: sqrt\n",
      "  max_samples: 0.9\n",
      "  min_impurity_decrease: 0.0001\n",
      "  min_samples_leaf: 2\n",
      "  min_samples_split: 2\n",
      "  n_estimators: 300\n",
      "\n",
      "Performance Metrics:\n",
      "  Validation F1: 0.9999\n",
      "  Test Accuracy:  1.0000\n",
      "  Test F1 Score:  1.0000\n",
      "  Test Precision: 1.0000\n",
      "  Test Recall:    1.0000\n",
      "\n",
      "EXPERIMENT COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "class RandomForestCNFExperiment:\n",
    "    \"\"\"\n",
    "    Experiment runner for Random Forest classification on CNF datasets.\n",
    "    Optimized for best accuracy and F1 scores with reasonable runtime.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./all_data'):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.results = []\n",
    "        \n",
    "        # Comprehensive hyperparameter distributions for best performance\n",
    "        self.param_distributions = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [None, 30, 50, 70],\n",
    "            'min_samples_split': [2, 5, 10, 15],\n",
    "            'min_samples_leaf': [1, 2, 4, 8],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'bootstrap': [True],\n",
    "            'max_samples': [0.8, 0.9, 1.0],\n",
    "            'min_impurity_decrease': [0.0, 0.0001, 0.001],\n",
    "            'class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "        }\n",
    "    \n",
    "    def discover_datasets(self):\n",
    "        available_datasets = []\n",
    "        clause_configs = [300, 500, 1000, 1500, 1800]\n",
    "        example_configs = [100, 1000, 5000]\n",
    "        \n",
    "        print(\"\\nScanning for available datasets...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for clauses in clause_configs:\n",
    "            for examples in example_configs:\n",
    "                base_name = f\"c{clauses}_d{examples}\"\n",
    "                train_file = self.data_dir / f\"train_{base_name}.csv\"\n",
    "                valid_file = self.data_dir / f\"valid_{base_name}.csv\"\n",
    "                test_file = self.data_dir / f\"test_{base_name}.csv\"\n",
    "                \n",
    "                if train_file.exists() and valid_file.exists() and test_file.exists():\n",
    "                    available_datasets.append((clauses, examples))\n",
    "                    print(f\"Found: {clauses} clauses, {examples} examples\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Total available datasets: {len(available_datasets)}/15\\n\")\n",
    "        \n",
    "        return available_datasets\n",
    "    \n",
    "    def load_dataset(self, clauses, examples):\n",
    "        base_name = f\"c{clauses}_d{examples}\"\n",
    "        \n",
    "        train_file = self.data_dir / f\"train_{base_name}.csv\"\n",
    "        valid_file = self.data_dir / f\"valid_{base_name}.csv\"\n",
    "        test_file = self.data_dir / f\"test_{base_name}.csv\"\n",
    "        \n",
    "        train_df = pd.read_csv(train_file, header=None)\n",
    "        valid_df = pd.read_csv(valid_file, header=None)\n",
    "        test_df = pd.read_csv(test_file, header=None)\n",
    "        \n",
    "        return {\n",
    "            'train': train_df,\n",
    "            'valid': valid_df,\n",
    "            'test': test_df\n",
    "        }\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        return X, y\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_valid, y_valid, examples):\n",
    "        print(\"  Tuning hyperparameters with extensive search...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # More iterations for better hyperparameter search\n",
    "        # Adaptive based on dataset size for efficiency\n",
    "        if examples == 100:\n",
    "            n_iter = 40\n",
    "            cv = 5\n",
    "        elif examples == 1000:\n",
    "            n_iter = 35\n",
    "            cv = 5\n",
    "        else:  # 5000\n",
    "            n_iter = 30\n",
    "            cv = 3  # Use 3-fold for large datasets\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=42, n_jobs=-1, warm_start=False)\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            rf,\n",
    "            self.param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        valid_pred = best_model.predict(X_valid)\n",
    "        valid_score = f1_score(y_valid, valid_pred)\n",
    "        valid_acc = accuracy_score(y_valid, valid_pred)\n",
    "        \n",
    "        # Also check training score to detect overfitting\n",
    "        train_pred = best_model.predict(X_train)\n",
    "        train_score = f1_score(y_train, train_pred)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"  Best CV F1: {random_search.best_score_:.4f}\")\n",
    "        print(f\"  Train F1: {train_score:.4f}, Validation F1: {valid_score:.4f}\")\n",
    "        print(f\"  Validation Accuracy: {valid_acc:.4f}\")\n",
    "        print(f\"  Tuning time: {elapsed_time:.1f}s\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        if train_score - valid_score > 0.1:\n",
    "            print(f\"  Note: Possible overfitting detected (train-valid gap: {train_score - valid_score:.4f})\")\n",
    "        \n",
    "        return random_search.best_params_, valid_score\n",
    "    \n",
    "    def train_final_model(self, X_train, y_train, X_valid, y_valid, best_params):\n",
    "        print(\"  Training final model on combined train+valid...\")\n",
    "        \n",
    "        X_combined = np.vstack([X_train, X_valid])\n",
    "        y_combined = np.hstack([y_train, y_valid])\n",
    "        \n",
    "        final_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
    "        final_model.fit(X_combined, y_combined)\n",
    "        \n",
    "        return final_model\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Additional metrics\n",
    "        from sklearn.metrics import precision_score, recall_score\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    def run_experiment(self, clauses, examples):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Experiment: {clauses} clauses, {examples} examples\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        data = self.load_dataset(clauses, examples)\n",
    "        \n",
    "        X_train, y_train = self.prepare_data(data['train'])\n",
    "        X_valid, y_valid = self.prepare_data(data['valid'])\n",
    "        X_test, y_test = self.prepare_data(data['test'])\n",
    "        \n",
    "        print(f\"  Train: {X_train.shape}, Valid: {X_valid.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        train_pos = np.sum(y_train) / len(y_train)\n",
    "        test_pos = np.sum(y_test) / len(y_test)\n",
    "        print(f\"  Class balance - Train: {train_pos:.2%} positive, Test: {test_pos:.2%} positive\")\n",
    "        \n",
    "        best_params, valid_score = self.tune_hyperparameters(\n",
    "            X_train, y_train, X_valid, y_valid, examples\n",
    "        )\n",
    "        \n",
    "        final_model = self.train_final_model(\n",
    "            X_train, y_train, X_valid, y_valid, best_params\n",
    "        )\n",
    "        \n",
    "        print(\"  Evaluating on test set...\")\n",
    "        test_metrics = self.evaluate_model(final_model, X_test, y_test)\n",
    "        \n",
    "        result = {\n",
    "            'clauses': clauses,\n",
    "            'examples': examples,\n",
    "            'n_features': X_train.shape[1],\n",
    "            'best_params': best_params,\n",
    "            'validation_f1': valid_score,\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_f1_score': test_metrics['f1_score'],\n",
    "            'test_precision': test_metrics['precision'],\n",
    "            'test_recall': test_metrics['recall']\n",
    "        }\n",
    "        \n",
    "        print(f\"  Test Results:\")\n",
    "        print(f\"    Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"    F1 Score:  {test_metrics['f1_score']:.4f}\")\n",
    "        print(f\"    Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"    Recall:    {test_metrics['recall']:.4f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        print(\"\\nRANDOM FOREST CLASSIFIER - CNF CLASSIFICATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Data directory: {self.data_dir.absolute()}\")\n",
    "        print(\"Focus: Best accuracy and F1 scores\")\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        available_datasets = self.discover_datasets()\n",
    "        \n",
    "        if not available_datasets:\n",
    "            print(\"\\nERROR: No complete datasets found!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Processing {len(available_datasets)} datasets\\n\")\n",
    "        \n",
    "        for idx, (clauses, examples) in enumerate(available_datasets, 1):\n",
    "            elapsed = time.time() - total_start_time\n",
    "            \n",
    "            print(f\"\\n[Dataset {idx}/{len(available_datasets)}] Elapsed: {elapsed/60:.1f}m\")\n",
    "            \n",
    "            try:\n",
    "                dataset_start = time.time()\n",
    "                result = self.run_experiment(clauses, examples)\n",
    "                self.results.append(result)\n",
    "                dataset_time = time.time() - dataset_start\n",
    "                print(f\"  Dataset completed in {dataset_time:.1f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Total execution time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"Completed {len(self.results)}/{len(available_datasets)} datasets\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if self.results:\n",
    "            self.save_results()\n",
    "            self.print_summary()\n",
    "        else:\n",
    "            print(\"No experiments completed.\")\n",
    "    \n",
    "    def save_results(self, filename='random_forest_results.json'):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"SUMMARY - RANDOM FOREST CLASSIFIER\")\n",
    "        print(\"=\"*90)\n",
    "        print(f\"{'Clauses':<10} {'Examples':<10} {'Features':<10} {'Test Acc':<12} {'Test F1':<12} {'Precision':<12} {'Recall':<12}\")\n",
    "        print(\"-\"*90)\n",
    "        \n",
    "        for result in self.results:\n",
    "            print(f\"{result['clauses']:<10} {result['examples']:<10} \"\n",
    "                  f\"{result['n_features']:<10} \"\n",
    "                  f\"{result['test_accuracy']:<12.4f} \"\n",
    "                  f\"{result['test_f1_score']:<12.4f} \"\n",
    "                  f\"{result['test_precision']:<12.4f} \"\n",
    "                  f\"{result['test_recall']:<12.4f}\")\n",
    "        \n",
    "        print(\"=\"*90)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BEST HYPERPARAMETERS FOR EACH CONFIGURATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for result in self.results:\n",
    "            print(f\"\\nConfiguration: {result['clauses']} clauses, {result['examples']} examples\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            print(\"Random Forest Parameters:\")\n",
    "            for param, value in sorted(result['best_params'].items()):\n",
    "                print(f\"  {param}: {value}\")\n",
    "            \n",
    "            print(f\"\\nPerformance Metrics:\")\n",
    "            print(f\"  Validation F1: {result['validation_f1']:.4f}\")\n",
    "            print(f\"  Test Accuracy:  {result['test_accuracy']:.4f}\")\n",
    "            print(f\"  Test F1 Score:  {result['test_f1_score']:.4f}\")\n",
    "            print(f\"  Test Precision: {result['test_precision']:.4f}\")\n",
    "            print(f\"  Test Recall:    {result['test_recall']:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"\\nEXPERIMENT 3: RANDOM FOREST CLASSIFIER\")\n",
    "    print(\"CNF Boolean Formula Classification\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    experiment = RandomForestCNFExperiment(data_dir='./all_data')\n",
    "    experiment.run_all_experiments()\n",
    "    \n",
    "    print(\"\\nEXPERIMENT COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0a86e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPERIMENT 5: GRADIENT BOOSTING CLASSIFIER\n",
      "CNF Boolean Formula Classification\n",
      "============================================================\n",
      "\n",
      "GRADIENT BOOSTING CLASSIFIER - CNF CLASSIFICATION\n",
      "============================================================\n",
      "Data directory: c:\\utd\\sem1\\machinelearning\\project2\\project2_data\\all_data\n",
      "Optimized for speed (<10 minutes target)\n",
      "\n",
      "Scanning for available datasets...\n",
      "------------------------------------------------------------\n",
      "✓ Found: 300 clauses, 100 examples\n",
      "✓ Found: 300 clauses, 1000 examples\n",
      "✓ Found: 300 clauses, 5000 examples\n",
      "✓ Found: 500 clauses, 100 examples\n",
      "✓ Found: 500 clauses, 1000 examples\n",
      "✓ Found: 500 clauses, 5000 examples\n",
      "✓ Found: 1000 clauses, 100 examples\n",
      "✓ Found: 1000 clauses, 1000 examples\n",
      "✓ Found: 1000 clauses, 5000 examples\n",
      "✓ Found: 1500 clauses, 100 examples\n",
      "✓ Found: 1500 clauses, 1000 examples\n",
      "✓ Found: 1500 clauses, 5000 examples\n",
      "✓ Found: 1800 clauses, 100 examples\n",
      "✓ Found: 1800 clauses, 1000 examples\n",
      "✓ Found: 1800 clauses, 5000 examples\n",
      "------------------------------------------------------------\n",
      "Total available datasets: 15/15\n",
      "\n",
      "Processing 15 datasets\n",
      "\n",
      "\n",
      "[1/15] Elapsed: 0.0m | ETA: 0.0m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 100 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(200, 500), Valid=(200, 500), Test=(200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.8038\n",
      "  Validation F1: 0.7512, Validation Acc: 0.7450\n",
      "  Tuning time: 1.5s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.8050\n",
      "    F1 Score:  0.8116\n",
      "    Precision: 0.7850\n",
      "    Recall:    0.8400\n",
      "  ✓ Completed in 1.9s\n",
      "\n",
      "[2/15] Elapsed: 0.0m | ETA: 0.2m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 1000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(2000, 500), Valid=(2000, 500), Test=(2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.8830\n",
      "  Validation F1: 0.8976, Validation Acc: 0.8955\n",
      "  Tuning time: 4.0s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9075\n",
      "    F1 Score:  0.9084\n",
      "    Precision: 0.8999\n",
      "    Recall:    0.9170\n",
      "  ✓ Completed in 5.6s\n",
      "\n",
      "[3/15] Elapsed: 0.1m | ETA: 0.5m\n",
      "\n",
      "============================================================\n",
      "Experiment: 300 clauses, 5000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(10000, 500), Valid=(10000, 500), Test=(10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9195\n",
      "  Validation F1: 0.9293, Validation Acc: 0.9283\n",
      "  Tuning time: 7.3s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9342\n",
      "    F1 Score:  0.9351\n",
      "    Precision: 0.9222\n",
      "    Recall:    0.9484\n",
      "  ✓ Completed in 13.2s\n",
      "\n",
      "[4/15] Elapsed: 0.3m | ETA: 1.0m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 100 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(200, 500), Valid=(200, 500), Test=(200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.8608\n",
      "  Validation F1: 0.8865, Validation Acc: 0.8950\n",
      "  Tuning time: 1.0s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9050\n",
      "    F1 Score:  0.9055\n",
      "    Precision: 0.9010\n",
      "    Recall:    0.9100\n",
      "  ✓ Completed in 1.2s\n",
      "\n",
      "[5/15] Elapsed: 0.4m | ETA: 0.7m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 1000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(2000, 500), Valid=(2000, 500), Test=(2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9367\n",
      "  Validation F1: 0.9429, Validation Acc: 0.9425\n",
      "  Tuning time: 3.3s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9545\n",
      "    F1 Score:  0.9547\n",
      "    Precision: 0.9513\n",
      "    Recall:    0.9580\n",
      "  ✓ Completed in 4.7s\n",
      "\n",
      "[6/15] Elapsed: 0.4m | ETA: 0.7m\n",
      "\n",
      "============================================================\n",
      "Experiment: 500 clauses, 5000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(10000, 500), Valid=(10000, 500), Test=(10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9642\n",
      "  Validation F1: 0.9677, Validation Acc: 0.9676\n",
      "  Tuning time: 6.8s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9725\n",
      "    F1 Score:  0.9727\n",
      "    Precision: 0.9652\n",
      "    Recall:    0.9804\n",
      "  ✓ Completed in 12.1s\n",
      "\n",
      "[7/15] Elapsed: 0.6m | ETA: 0.7m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 100 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(200, 500), Valid=(200, 500), Test=(200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9949\n",
      "  Validation F1: 0.9950, Validation Acc: 0.9950\n",
      "  Tuning time: 1.1s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  ✓ Completed in 1.7s\n",
      "\n",
      "[8/15] Elapsed: 0.7m | ETA: 0.6m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 1000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(2000, 500), Valid=(2000, 500), Test=(2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9970\n",
      "  Validation F1: 0.9965, Validation Acc: 0.9965\n",
      "  Tuning time: 3.6s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9945\n",
      "    F1 Score:  0.9945\n",
      "    Precision: 0.9930\n",
      "    Recall:    0.9960\n",
      "  ✓ Completed in 5.3s\n",
      "\n",
      "[9/15] Elapsed: 0.8m | ETA: 0.5m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1000 clauses, 5000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(10000, 500), Valid=(10000, 500), Test=(10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 0.9974\n",
      "  Validation F1: 0.9988, Validation Acc: 0.9988\n",
      "  Tuning time: 6.6s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  0.9981\n",
      "    F1 Score:  0.9981\n",
      "    Precision: 0.9980\n",
      "    Recall:    0.9982\n",
      "  ✓ Completed in 11.8s\n",
      "\n",
      "[10/15] Elapsed: 1.0m | ETA: 0.5m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 100 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(200, 500), Valid=(200, 500), Test=(200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 1.0000\n",
      "  Validation F1: 1.0000, Validation Acc: 1.0000\n",
      "  Tuning time: 1.1s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  ✓ Completed in 1.7s\n",
      "\n",
      "[11/15] Elapsed: 1.0m | ETA: 0.4m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 1000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(2000, 500), Valid=(2000, 500), Test=(2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 1.0000\n",
      "  Validation F1: 1.0000, Validation Acc: 1.0000\n",
      "  Tuning time: 3.4s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  ✓ Completed in 4.9s\n",
      "\n",
      "[12/15] Elapsed: 1.1m | ETA: 0.3m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1500 clauses, 5000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(10000, 500), Valid=(10000, 500), Test=(10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 1.0000\n",
      "  Validation F1: 1.0000, Validation Acc: 1.0000\n",
      "  Tuning time: 6.2s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  ✓ Completed in 10.9s\n",
      "\n",
      "[13/15] Elapsed: 1.3m | ETA: 0.2m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 100 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(200, 500), Valid=(200, 500), Test=(200, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 1.0000\n",
      "  Validation F1: 1.0000, Validation Acc: 1.0000\n",
      "  Tuning time: 1.1s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  ✓ Completed in 1.5s\n",
      "\n",
      "[14/15] Elapsed: 1.3m | ETA: 0.1m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 1000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(2000, 500), Valid=(2000, 500), Test=(2000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 1.0000\n",
      "  Validation F1: 1.0000, Validation Acc: 1.0000\n",
      "  Tuning time: 3.2s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  ✓ Completed in 4.6s\n",
      "\n",
      "[15/15] Elapsed: 1.4m | ETA: 0.0m\n",
      "\n",
      "============================================================\n",
      "Experiment: 1800 clauses, 5000 examples\n",
      "============================================================\n",
      "  Dataset shapes: Train=(10000, 500), Valid=(10000, 500), Test=(10000, 500)\n",
      "  Tuning hyperparameters...\n",
      "  Best CV F1: 1.0000\n",
      "  Validation F1: 1.0000, Validation Acc: 1.0000\n",
      "  Tuning time: 6.2s\n",
      "  Training final model on combined train+valid...\n",
      "  Evaluating on test set...\n",
      "  Test Results:\n",
      "    Accuracy:  1.0000\n",
      "    F1 Score:  1.0000\n",
      "    Precision: 1.0000\n",
      "    Recall:    1.0000\n",
      "  ✓ Completed in 10.8s\n",
      "\n",
      "============================================================\n",
      "Total execution time: 1.53 minutes\n",
      "Completed 15/15 datasets\n",
      "============================================================\n",
      "\n",
      "Results saved to gradient_boosting_results.json\n",
      "\n",
      "==========================================================================================\n",
      "SUMMARY - GRADIENT BOOSTING CLASSIFIER\n",
      "==========================================================================================\n",
      "Clauses    Examples   Features   Test Acc     Test F1      Precision    Recall      \n",
      "------------------------------------------------------------------------------------------\n",
      "300        100        500        0.8050       0.8116       0.7850       0.8400      \n",
      "300        1000       500        0.9075       0.9084       0.8999       0.9170      \n",
      "300        5000       500        0.9342       0.9351       0.9222       0.9484      \n",
      "500        100        500        0.9050       0.9055       0.9010       0.9100      \n",
      "500        1000       500        0.9545       0.9547       0.9513       0.9580      \n",
      "500        5000       500        0.9725       0.9727       0.9652       0.9804      \n",
      "1000       100        500        1.0000       1.0000       1.0000       1.0000      \n",
      "1000       1000       500        0.9945       0.9945       0.9930       0.9960      \n",
      "1000       5000       500        0.9981       0.9981       0.9980       0.9982      \n",
      "1500       100        500        1.0000       1.0000       1.0000       1.0000      \n",
      "1500       1000       500        1.0000       1.0000       1.0000       1.0000      \n",
      "1500       5000       500        1.0000       1.0000       1.0000       1.0000      \n",
      "1800       100        500        1.0000       1.0000       1.0000       1.0000      \n",
      "1800       1000       500        1.0000       1.0000       1.0000       1.0000      \n",
      "1800       5000       500        1.0000       1.0000       1.0000       1.0000      \n",
      "==========================================================================================\n",
      "\n",
      "Average                          0.9648       0.9654      \n",
      "==========================================================================================\n",
      "\n",
      "================================================================================\n",
      "BEST HYPERPARAMETERS FOR EACH CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "300 clauses, 100 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.1, max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0\n",
      "  Performance: Acc=0.8050, F1=0.8116, Prec=0.7850, Rec=0.8400\n",
      "\n",
      "300 clauses, 1000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.05, max_depth=5, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=0.9075, F1=0.9084, Prec=0.8999, Rec=0.9170\n",
      "\n",
      "300 clauses, 5000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.1, max_depth=7, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=150, subsample=1.0\n",
      "  Performance: Acc=0.9342, F1=0.9351, Prec=0.9222, Rec=0.9484\n",
      "\n",
      "500 clauses, 100 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.2, max_depth=3, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=0.9050, F1=0.9055, Prec=0.9010, Rec=0.9100\n",
      "\n",
      "500 clauses, 1000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.05, max_depth=5, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=0.9545, F1=0.9547, Prec=0.9513, Rec=0.9580\n",
      "\n",
      "500 clauses, 5000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.1, max_depth=7, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=150, subsample=1.0\n",
      "  Performance: Acc=0.9725, F1=0.9727, Prec=0.9652, Rec=0.9804\n",
      "\n",
      "1000 clauses, 100 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.1, max_depth=7, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=150, subsample=1.0\n",
      "  Performance: Acc=1.0000, F1=1.0000, Prec=1.0000, Rec=1.0000\n",
      "\n",
      "1000 clauses, 1000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.1, max_depth=7, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=150, subsample=1.0\n",
      "  Performance: Acc=0.9945, F1=0.9945, Prec=0.9930, Rec=0.9960\n",
      "\n",
      "1000 clauses, 5000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.1, max_depth=7, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=150, subsample=1.0\n",
      "  Performance: Acc=0.9981, F1=0.9981, Prec=0.9980, Rec=0.9982\n",
      "\n",
      "1500 clauses, 100 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.2, max_depth=7, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=1.0000, F1=1.0000, Prec=1.0000, Rec=1.0000\n",
      "\n",
      "1500 clauses, 1000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.2, max_depth=7, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=1.0000, F1=1.0000, Prec=1.0000, Rec=1.0000\n",
      "\n",
      "1500 clauses, 5000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.2, max_depth=7, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=1.0000, F1=1.0000, Prec=1.0000, Rec=1.0000\n",
      "\n",
      "1800 clauses, 100 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.2, max_depth=7, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=1.0000, F1=1.0000, Prec=1.0000, Rec=1.0000\n",
      "\n",
      "1800 clauses, 1000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.2, max_depth=7, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=1.0000, F1=1.0000, Prec=1.0000, Rec=1.0000\n",
      "\n",
      "1800 clauses, 5000 examples:\n",
      "--------------------------------------------------------------------------------\n",
      "  Parameters: learning_rate=0.2, max_depth=7, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=150, subsample=0.8\n",
      "  Performance: Acc=1.0000, F1=1.0000, Prec=1.0000, Rec=1.0000\n",
      "\n",
      "EXPERIMENT COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "class GradientBoostingCNFExperiment:\n",
    "    \"\"\"\n",
    "    Experiment runner for Gradient Boosting classification on CNF datasets.\n",
    "    Optimized for speed while maintaining good performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./all_data'):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.results = []\n",
    "        \n",
    "        # Reduced hyperparameter space for faster search\n",
    "        self.param_distributions = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 10],\n",
    "            'min_samples_leaf': [1, 4],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'subsample': [0.8, 1.0],\n",
    "        }\n",
    "    \n",
    "    def discover_datasets(self):\n",
    "        available_datasets = []\n",
    "        clause_configs = [300, 500, 1000, 1500, 1800]\n",
    "        example_configs = [100, 1000, 5000]\n",
    "        \n",
    "        print(\"\\nScanning for available datasets...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for clauses in clause_configs:\n",
    "            for examples in example_configs:\n",
    "                base_name = f\"c{clauses}_d{examples}\"\n",
    "                train_file = self.data_dir / f\"train_{base_name}.csv\"\n",
    "                valid_file = self.data_dir / f\"valid_{base_name}.csv\"\n",
    "                test_file = self.data_dir / f\"test_{base_name}.csv\"\n",
    "                \n",
    "                if train_file.exists() and valid_file.exists() and test_file.exists():\n",
    "                    available_datasets.append((clauses, examples))\n",
    "                    print(f\"✓ Found: {clauses} clauses, {examples} examples\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Total available datasets: {len(available_datasets)}/15\\n\")\n",
    "        \n",
    "        return available_datasets\n",
    "    \n",
    "    def load_dataset(self, clauses, examples):\n",
    "        base_name = f\"c{clauses}_d{examples}\"\n",
    "        \n",
    "        train_file = self.data_dir / f\"train_{base_name}.csv\"\n",
    "        valid_file = self.data_dir / f\"valid_{base_name}.csv\"\n",
    "        test_file = self.data_dir / f\"test_{base_name}.csv\"\n",
    "        \n",
    "        train_df = pd.read_csv(train_file, header=None)\n",
    "        valid_df = pd.read_csv(valid_file, header=None)\n",
    "        test_df = pd.read_csv(test_file, header=None)\n",
    "        \n",
    "        return {\n",
    "            'train': train_df,\n",
    "            'valid': valid_df,\n",
    "            'test': test_df\n",
    "        }\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        return X, y\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_valid, y_valid, examples):\n",
    "        print(\"  Tuning hyperparameters...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Significantly reduced iterations for speed\n",
    "        if examples == 100:\n",
    "            n_iter = 15\n",
    "            cv = 3\n",
    "        elif examples == 1000:\n",
    "            n_iter = 12\n",
    "            cv = 3\n",
    "        else:  # 5000\n",
    "            n_iter = 10\n",
    "            cv = 2\n",
    "        \n",
    "        gb = GradientBoostingClassifier(random_state=42, verbose=0)\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            gb,\n",
    "            self.param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "            return_train_score=False  # Don't compute train scores to save time\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        valid_pred = best_model.predict(X_valid)\n",
    "        valid_score = f1_score(y_valid, valid_pred)\n",
    "        valid_acc = accuracy_score(y_valid, valid_pred)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"  Best CV F1: {random_search.best_score_:.4f}\")\n",
    "        print(f\"  Validation F1: {valid_score:.4f}, Validation Acc: {valid_acc:.4f}\")\n",
    "        print(f\"  Tuning time: {elapsed_time:.1f}s\")\n",
    "        \n",
    "        return random_search.best_params_, valid_score\n",
    "    \n",
    "    def train_final_model(self, X_train, y_train, X_valid, y_valid, best_params):\n",
    "        print(\"  Training final model on combined train+valid...\")\n",
    "        \n",
    "        X_combined = np.vstack([X_train, X_valid])\n",
    "        y_combined = np.hstack([y_train, y_valid])\n",
    "        \n",
    "        final_model = GradientBoostingClassifier(**best_params, random_state=42, verbose=0)\n",
    "        final_model.fit(X_combined, y_combined)\n",
    "        \n",
    "        return final_model\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Additional metrics\n",
    "        from sklearn.metrics import precision_score, recall_score\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    def run_experiment(self, clauses, examples):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Experiment: {clauses} clauses, {examples} examples\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        data = self.load_dataset(clauses, examples)\n",
    "        \n",
    "        X_train, y_train = self.prepare_data(data['train'])\n",
    "        X_valid, y_valid = self.prepare_data(data['valid'])\n",
    "        X_test, y_test = self.prepare_data(data['test'])\n",
    "        \n",
    "        print(f\"  Dataset shapes: Train={X_train.shape}, Valid={X_valid.shape}, Test={X_test.shape}\")\n",
    "        \n",
    "        best_params, valid_score = self.tune_hyperparameters(\n",
    "            X_train, y_train, X_valid, y_valid, examples\n",
    "        )\n",
    "        \n",
    "        final_model = self.train_final_model(\n",
    "            X_train, y_train, X_valid, y_valid, best_params\n",
    "        )\n",
    "        \n",
    "        print(\"  Evaluating on test set...\")\n",
    "        test_metrics = self.evaluate_model(final_model, X_test, y_test)\n",
    "        \n",
    "        result = {\n",
    "            'clauses': clauses,\n",
    "            'examples': examples,\n",
    "            'n_features': X_train.shape[1],\n",
    "            'best_params': best_params,\n",
    "            'validation_f1': valid_score,\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_f1_score': test_metrics['f1_score'],\n",
    "            'test_precision': test_metrics['precision'],\n",
    "            'test_recall': test_metrics['recall']\n",
    "        }\n",
    "        \n",
    "        print(f\"  Test Results:\")\n",
    "        print(f\"    Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"    F1 Score:  {test_metrics['f1_score']:.4f}\")\n",
    "        print(f\"    Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"    Recall:    {test_metrics['recall']:.4f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        print(\"\\nGRADIENT BOOSTING CLASSIFIER - CNF CLASSIFICATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Data directory: {self.data_dir.absolute()}\")\n",
    "        print(\"Optimized for speed (<10 minutes target)\")\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        available_datasets = self.discover_datasets()\n",
    "        \n",
    "        if not available_datasets:\n",
    "            print(\"\\nERROR: No complete datasets found!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Processing {len(available_datasets)} datasets\\n\")\n",
    "        \n",
    "        for idx, (clauses, examples) in enumerate(available_datasets, 1):\n",
    "            elapsed = time.time() - total_start_time\n",
    "            \n",
    "            print(f\"\\n[{idx}/{len(available_datasets)}] Elapsed: {elapsed/60:.1f}m | \"\n",
    "                  f\"ETA: {(elapsed/idx)*(len(available_datasets)-idx)/60:.1f}m\")\n",
    "            \n",
    "            try:\n",
    "                dataset_start = time.time()\n",
    "                result = self.run_experiment(clauses, examples)\n",
    "                self.results.append(result)\n",
    "                dataset_time = time.time() - dataset_start\n",
    "                print(f\"  ✓ Completed in {dataset_time:.1f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Total execution time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"Completed {len(self.results)}/{len(available_datasets)} datasets\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if self.results:\n",
    "            self.save_results()\n",
    "            self.print_summary()\n",
    "        else:\n",
    "            print(\"No experiments completed.\")\n",
    "    \n",
    "    def save_results(self, filename='gradient_boosting_results.json'):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"SUMMARY - GRADIENT BOOSTING CLASSIFIER\")\n",
    "        print(\"=\"*90)\n",
    "        print(f\"{'Clauses':<10} {'Examples':<10} {'Features':<10} {'Test Acc':<12} {'Test F1':<12} {'Precision':<12} {'Recall':<12}\")\n",
    "        print(\"-\"*90)\n",
    "        \n",
    "        for result in self.results:\n",
    "            print(f\"{result['clauses']:<10} {result['examples']:<10} \"\n",
    "                  f\"{result['n_features']:<10} \"\n",
    "                  f\"{result['test_accuracy']:<12.4f} \"\n",
    "                  f\"{result['test_f1_score']:<12.4f} \"\n",
    "                  f\"{result['test_precision']:<12.4f} \"\n",
    "                  f\"{result['test_recall']:<12.4f}\")\n",
    "        \n",
    "        print(\"=\"*90)\n",
    "        \n",
    "        # Calculate and display averages\n",
    "        avg_acc = np.mean([r['test_accuracy'] for r in self.results])\n",
    "        avg_f1 = np.mean([r['test_f1_score'] for r in self.results])\n",
    "        print(f\"\\n{'Average':<10} {'':<10} {'':<10} {avg_acc:<12.4f} {avg_f1:<12.4f}\")\n",
    "        print(\"=\"*90)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BEST HYPERPARAMETERS FOR EACH CONFIGURATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for result in self.results:\n",
    "            print(f\"\\n{result['clauses']} clauses, {result['examples']} examples:\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            params_str = \", \".join([f\"{k}={v}\" for k, v in sorted(result['best_params'].items())])\n",
    "            print(f\"  Parameters: {params_str}\")\n",
    "            \n",
    "            print(f\"  Performance: Acc={result['test_accuracy']:.4f}, \"\n",
    "                  f\"F1={result['test_f1_score']:.4f}, \"\n",
    "                  f\"Prec={result['test_precision']:.4f}, \"\n",
    "                  f\"Rec={result['test_recall']:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"\\nEXPERIMENT 5: GRADIENT BOOSTING CLASSIFIER\")\n",
    "    print(\"CNF Boolean Formula Classification\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    experiment = GradientBoostingCNFExperiment(data_dir='./all_data')\n",
    "    experiment.run_all_experiments()\n",
    "    \n",
    "    print(\"\\nEXPERIMENT COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3395055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING AND PREPROCESSING MNIST DATASET\n",
      "======================================================================\n",
      "Dataset loaded in 5.0s\n",
      "Training set: (60000, 784)\n",
      "Test set: (10000, 784)\n",
      "\n",
      "Starting experiments...\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 1: DECISION TREE CLASSIFIER\n",
      "======================================================================\n",
      "Testing: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.7939\n",
      "Testing: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7434\n",
      "Testing: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.7949\n",
      "Testing: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7497\n",
      "Testing: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.7937\n",
      "Testing: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7391\n",
      "Testing: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8044\n",
      "Testing: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7356\n",
      "Testing: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8324\n",
      "Testing: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7891\n",
      "Testing: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8443\n",
      "Testing: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7915\n",
      "Testing: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8415\n",
      "Testing: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.8042\n",
      "Testing: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8396\n",
      "Testing: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.8028\n",
      "Testing: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8369\n",
      "Testing: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7936\n",
      "Testing: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8384\n",
      "Testing: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7876\n",
      "Testing: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8405\n",
      "Testing: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7919\n",
      "Testing: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8396\n",
      "Testing: {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7983\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.7904\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7487\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8096\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7372\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.7910\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7200\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.7903\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7435\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8353\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.8000\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8360\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7956\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8318\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7955\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8400\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7948\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8429\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7978\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8360\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.7974\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8437\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.8089\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.8412\n",
      "Testing: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}\n",
      " → Accuracy: 0.8017\n",
      "\n",
      "Best Decision Tree Accuracy: 0.8443\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 2: BAGGING CLASSIFIER\n",
      "======================================================================\n",
      "Testing: base={'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}, bag={'n_estimators': 25, 'max_samples': 0.9, 'max_features': 0.9}\n",
      " → Accuracy: 0.9362\n",
      "Testing: base={'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}, bag={'n_estimators': 25, 'max_samples': 0.9, 'max_features': 0.9}\n",
      " → Accuracy: 0.9362\n",
      "Testing: base={'criterion': 'gini', 'max_depth': 25, 'min_samples_split': 5}, bag={'n_estimators': 25, 'max_samples': 0.9, 'max_features': 0.9}\n",
      " → Accuracy: 0.9539\n",
      "Testing: base={'criterion': 'gini', 'max_depth': 25, 'min_samples_split': 10}, bag={'n_estimators': 25, 'max_samples': 0.9, 'max_features': 0.9}\n",
      " → Accuracy: 0.9530\n",
      "\n",
      "Best Bagging Accuracy: 0.9539\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 3: RANDOM FOREST CLASSIFIER\n",
      "======================================================================\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9676\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9620\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9664\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9588\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9649\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9601\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9641\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9586\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9672\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9632\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9676\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9619\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9659\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9615\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9648\n",
      "Testing: {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9593\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9665\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9634\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9659\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9607\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9650\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9614\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9659\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9602\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9667\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9654\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9664\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9612\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9652\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9618\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9651\n",
      "Testing: {'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
      " → Accuracy: 0.9615\n",
      "\n",
      "Best Random Forest Accuracy: 0.9676\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 4: GRADIENT BOOSTING CLASSIFIER\n",
      "======================================================================\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9086\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9135\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9089\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9139\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9103\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9136\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9094\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9142\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9381\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9395\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9391\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9376\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9361\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9382\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9381\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9391\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9475\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9505\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9472\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9492\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9470\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9483\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9485\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9512\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9632\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9649\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9645\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9630\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9629\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9642\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.8, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9633\n",
      "Testing: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.9, 'max_features': 'sqrt'}\n",
      " → Accuracy: 0.9651\n",
      "\n",
      "Best Gradient Boosting Accuracy: 0.9651\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY - ALL CLASSIFIERS ON MNIST\n",
      "================================================================================\n",
      "\n",
      "Classifier                Test Accuracy        Time (min)     \n",
      "--------------------------------------------------------------------------------\n",
      "Decision Tree             0.8443               0.1            \n",
      "Bagging                   0.9539               8.0            \n",
      "Random Forest             0.9676               0.3            \n",
      "Gradient Boosting         0.9651               37.9           \n",
      "================================================================================\n",
      "\n",
      "Best Classifier: Random Forest (0.9676)\n",
      "Total runtime: 58.3 minutes\n",
      "\n",
      "Results saved to mnist_results.json\n",
      "\n",
      "ALL EXPERIMENTS COMPLETED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class MNISTClassifierExperiment:\n",
    "    \"\"\"Evaluate Decision Tree, Bagging, Random Forest, and Gradient Boosting on MNIST with param grids.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Data Loading\n",
    "    # -----------------------------------------------------\n",
    "    def load_and_preprocess_data(self):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"LOADING AND PREPROCESSING MNIST DATASET\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, parser=\"auto\")\n",
    "        X = np.array(X, dtype=np.float32) / 255.0\n",
    "        y = np.array(y)\n",
    "\n",
    "        self.X_train, self.X_test = X[:60000], X[60000:]\n",
    "        self.y_train, self.y_test = y[:60000], y[60000:]\n",
    "\n",
    "        del X, y\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"Dataset loaded in {time.time() - start_time:.1f}s\")\n",
    "        print(f\"Training set: {self.X_train.shape}\")\n",
    "        print(f\"Test set: {self.X_test.shape}\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Helper to loop through parameter grids\n",
    "    # -----------------------------------------------------\n",
    "    def generate_param_combinations(self, grid_dict):\n",
    "        keys = grid_dict.keys()\n",
    "        values = grid_dict.values()\n",
    "        for combo in itertools.product(*values):\n",
    "            yield dict(zip(keys, combo))\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Experiment 1: Decision Tree\n",
    "    # -----------------------------------------------------\n",
    "    def experiment_1_decision_tree(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPERIMENT 1: DECISION TREE CLASSIFIER\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        start_time = time.time()\n",
    "        grid = {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [10, 20, 30],\n",
    "            \"min_samples_split\": [5, 10],\n",
    "            \"min_samples_leaf\": [2, 4],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        }\n",
    "\n",
    "        best_result = None\n",
    "        for params in self.generate_param_combinations(grid):\n",
    "            print(f\"Testing: {params}\")\n",
    "            model = DecisionTreeClassifier(**params, random_state=42)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            acc = accuracy_score(self.y_test, model.predict(self.X_test))\n",
    "            print(f\" → Accuracy: {acc:.4f}\")\n",
    "            if not best_result or acc > best_result[\"test_accuracy\"]:\n",
    "                best_result = {\n",
    "                    \"classifier\": \"Decision Tree\",\n",
    "                    \"best_params\": params,\n",
    "                    \"test_accuracy\": float(acc),\n",
    "                    \"training_time\": time.time() - start_time,\n",
    "                }\n",
    "\n",
    "        print(f\"\\nBest Decision Tree Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        self.results.append(best_result)\n",
    "        gc.collect()\n",
    "        return best_result\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Experiment 2: Bagging Classifier\n",
    "    # -----------------------------------------------------\n",
    "    def experiment_2_bagging(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPERIMENT 2: BAGGING CLASSIFIER\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        start_time = time.time()\n",
    "        bagging_grid = {\n",
    "            \"n_estimators\": [25],\n",
    "            \"max_samples\": [ 0.9],\n",
    "            \"max_features\": [ 0.9],\n",
    "        }\n",
    "        base_tree_grid = {\n",
    "            \"criterion\": [\"gini\"],\n",
    "            \"max_depth\": [10, 25],\n",
    "            \"min_samples_split\": [5, 10],\n",
    "        }\n",
    "\n",
    "        best_result = None\n",
    "        for tree_params in self.generate_param_combinations(base_tree_grid):\n",
    "            for bag_params in self.generate_param_combinations(bagging_grid):\n",
    "                print(f\"Testing: base={tree_params}, bag={bag_params}\")\n",
    "                base = DecisionTreeClassifier(**tree_params, random_state=42)\n",
    "                bag = BaggingClassifier(\n",
    "                    estimator=base, random_state=42, n_jobs=1, **bag_params\n",
    "                )\n",
    "                bag.fit(self.X_train, self.y_train)\n",
    "                acc = accuracy_score(self.y_test, bag.predict(self.X_test))\n",
    "                print(f\" → Accuracy: {acc:.4f}\")\n",
    "                if not best_result or acc > best_result[\"test_accuracy\"]:\n",
    "                    best_result = {\n",
    "                        \"classifier\": \"Bagging\",\n",
    "                        \"best_params\": {**tree_params, **bag_params},\n",
    "                        \"test_accuracy\": float(acc),\n",
    "                        \"training_time\": time.time() - start_time,\n",
    "                    }\n",
    "\n",
    "        print(f\"\\nBest Bagging Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        self.results.append(best_result)\n",
    "        gc.collect()\n",
    "        return best_result\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Experiment 3: Random Forest\n",
    "    # -----------------------------------------------------\n",
    "    def experiment_3_random_forest(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPERIMENT 3: RANDOM FOREST CLASSIFIER\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        start_time = time.time()\n",
    "        grid = {\n",
    "            \"n_estimators\": [50],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [20, 30],\n",
    "            \"min_samples_split\": [5, 10],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        }\n",
    "\n",
    "        best_result = None\n",
    "        for params in self.generate_param_combinations(grid):\n",
    "            print(f\"Testing: {params}\")\n",
    "            rf = RandomForestClassifier(**params, n_jobs=1, random_state=42)\n",
    "            rf.fit(self.X_train, self.y_train)\n",
    "            acc = accuracy_score(self.y_test, rf.predict(self.X_test))\n",
    "            print(f\" → Accuracy: {acc:.4f}\")\n",
    "            if not best_result or acc > best_result[\"test_accuracy\"]:\n",
    "                best_result = {\n",
    "                    \"classifier\": \"Random Forest\",\n",
    "                    \"best_params\": params,\n",
    "                    \"test_accuracy\": float(acc),\n",
    "                    \"training_time\": time.time() - start_time,\n",
    "                }\n",
    "\n",
    "        print(f\"\\nBest Random Forest Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        self.results.append(best_result)\n",
    "        gc.collect()\n",
    "        return best_result\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Experiment 4: Gradient Boosting\n",
    "    # -----------------------------------------------------\n",
    "    def experiment_4_gradient_boosting(self):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPERIMENT 4: GRADIENT BOOSTING CLASSIFIER\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        start_time = time.time()\n",
    "        grid = {\n",
    "            \"n_estimators\": [50],\n",
    "            \"learning_rate\": [0.01, 0.1],\n",
    "            \"max_depth\": [5, 7],\n",
    "            \"min_samples_split\": [5, 10],\n",
    "            \"min_samples_leaf\": [2, 4],\n",
    "            \"subsample\": [0.8, 0.9],\n",
    "            \"max_features\": [\"sqrt\"],\n",
    "        }\n",
    "\n",
    "        best_result = None\n",
    "        for params in self.generate_param_combinations(grid):\n",
    "            print(f\"Testing: {params}\")\n",
    "            gb = GradientBoostingClassifier(**params, random_state=42)\n",
    "            gb.fit(self.X_train, self.y_train)\n",
    "            acc = accuracy_score(self.y_test, gb.predict(self.X_test))\n",
    "            print(f\" → Accuracy: {acc:.4f}\")\n",
    "            if not best_result or acc > best_result[\"test_accuracy\"]:\n",
    "                best_result = {\n",
    "                    \"classifier\": \"Gradient Boosting\",\n",
    "                    \"best_params\": params,\n",
    "                    \"test_accuracy\": float(acc),\n",
    "                    \"training_time\": time.time() - start_time,\n",
    "                }\n",
    "\n",
    "        print(f\"\\nBest Gradient Boosting Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        self.results.append(best_result)\n",
    "        gc.collect()\n",
    "        return best_result\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Run All Experiments\n",
    "    # -----------------------------------------------------\n",
    "    def run_all_experiments(self):\n",
    "        total_start = time.time()\n",
    "        self.load_and_preprocess_data()\n",
    "\n",
    "        print(\"\\nStarting experiments...\")\n",
    "        self.experiment_1_decision_tree()\n",
    "        self.experiment_2_bagging()\n",
    "        self.experiment_3_random_forest()\n",
    "        self.experiment_4_gradient_boosting()\n",
    "\n",
    "        total_time = time.time() - total_start\n",
    "        self.print_final_summary(total_time)\n",
    "        self.save_results()\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Summary\n",
    "    # -----------------------------------------------------\n",
    "    def print_final_summary(self, total_time):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"FINAL SUMMARY - ALL CLASSIFIERS ON MNIST\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n{'Classifier':<25} {'Test Accuracy':<20} {'Time (min)':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for r in self.results:\n",
    "            print(\n",
    "                f\"{r['classifier']:<25} \"\n",
    "                f\"{r['test_accuracy']:<20.4f} \"\n",
    "                f\"{r['training_time']/60:<15.1f}\"\n",
    "            )\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        best = max(self.results, key=lambda x: x[\"test_accuracy\"])\n",
    "        print(f\"\\nBest Classifier: {best['classifier']} ({best['test_accuracy']:.4f})\")\n",
    "        print(f\"Total runtime: {total_time/60:.1f} minutes\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Save Results\n",
    "    # -----------------------------------------------------\n",
    "    def save_results(self):\n",
    "        with open(\"mnist_results.json\", \"w\") as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(\"\\nResults saved to mnist_results.json\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# MAIN\n",
    "# -----------------------------------------------------\n",
    "def main():\n",
    "    experiment = MNISTClassifierExperiment()\n",
    "    experiment.run_all_experiments()\n",
    "    print(\"\\nALL EXPERIMENTS COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981a68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
